package com.example.minilmwrapper;

import android.content.Context;
import ai.onnxruntime.*;
import java.io.InputStream;
import java.io.IOException;
import java.util.HashMap;
import java.util.Map;
import java.util.ArrayList;
import java.util.List;
import java.util.Arrays;

public class SentenceEmbedder {
    private Context context;
    private OrtEnvironment env;
    private OrtSession session;
    private static final int MAX_SEQUENCE_LENGTH = 512;
    private static final int VOCAB_SIZE = 30522;
    
    // Simple tokenizer vocabulary (subset for demo)
    private Map<String, Integer> vocab;

    public SentenceEmbedder(Context context) {
        this.context = context;
        initVocab();
        try {
            env = OrtEnvironment.getEnvironment();
            byte[] modelBytes = loadModelFromAssets();
            session = env.createSession(modelBytes);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }

    private void initVocab() {
        vocab = new HashMap<>();
        vocab.put("[PAD]", 0);
        vocab.put("[UNK]", 100);
        vocab.put("[CLS]", 101);
        vocab.put("[SEP]", 102);
        vocab.put("the", 1996);
        vocab.put("a", 1037);
        vocab.put("an", 2019);
        vocab.put("and", 1998);
        vocab.put("or", 2030);
        vocab.put("but", 2021);
        vocab.put("in", 1999);
        vocab.put("on", 2006);
        vocab.put("at", 2012);
        vocab.put("to", 2000);
        vocab.put("for", 2005);
        vocab.put("of", 1997);
        vocab.put("with", 2007);
        vocab.put("by", 2011);
    }

    public float[] embed(String text) {
        try {
            long[] tokens = tokenize(text);
            long[] attentionMask = createAttentionMask(tokens);
            
            // Create input tensors
            OnnxTensor inputIds = OnnxTensor.createTensor(env, new long[][]{tokens});
            OnnxTensor attentionMaskTensor = OnnxTensor.createTensor(env, new long[][]{attentionMask});
            
            // Run inference
            Map<String, OnnxTensor> inputs = new HashMap<>();
            inputs.put("input_ids", inputIds);
            inputs.put("attention_mask", attentionMaskTensor);
            
            OrtSession.Result results = session.run(inputs);
            float[][] sentenceEmbedding = (float[][]) results.get("sentence_embedding").getValue();
            
            // Clean up tensors
            inputIds.close();
            attentionMaskTensor.close();
            results.close();
            
            return sentenceEmbedding[0];
        } catch (Exception e) {
            e.printStackTrace();
            // Fallback to dummy embedding if real inference fails
            return generateDummyEmbedding(text);
        }
    }

    private long[] tokenize(String text) {
        String[] words = text.toLowerCase().split("\\W+");
        List<Long> tokens = new ArrayList<>();
        
        // Add [CLS] token
        tokens.add((long) vocab.getOrDefault("[CLS]", 101));
        
        // Tokenize words
        for (String word : words) {
            if (!word.isEmpty()) {
                long tokenId = vocab.getOrDefault(word, vocab.getOrDefault("[UNK]", 100)).longValue();
                tokens.add(tokenId);
                
                if (tokens.size() >= MAX_SEQUENCE_LENGTH - 1) break;
            }
        }
        
        // Add [SEP] token
        tokens.add((long) vocab.getOrDefault("[SEP]", 102));
        
        // Pad to max length
        while (tokens.size() < MAX_SEQUENCE_LENGTH) {
            tokens.add((long) vocab.getOrDefault("[PAD]", 0));
        }
        
        return tokens.stream().mapToLong(Long::longValue).toArray();
    }

    private float[] generateDummyEmbedding(String text) {
        // Generate a deterministic but varied embedding based on text content
        float[] embedding = new float[384];
        int hash = text.hashCode();
        
        for (int i = 0; i < embedding.length; i++) {
            // Create pseudo-random but deterministic values
            int seed = (hash + i * 31) % 10000;
            embedding[i] = (seed / 10000.0f - 0.5f) * 2.0f; // Range: -1 to 1
        }
        
        // Normalize the embedding
        float norm = 0;
        for (float v : embedding) {
            norm += v * v;
        }
        norm = (float) Math.sqrt(norm);
        
        if (norm > 0) {
            for (int i = 0; i < embedding.length; i++) {
                embedding[i] = embedding[i] / norm;
            }
        }
        
        return embedding;
    }

    private long[] createAttentionMask(long[] tokens) {
        long[] mask = new long[tokens.length];
        long padToken = vocab.getOrDefault("[PAD]", 0).longValue();
        
        for (int i = 0; i < tokens.length; i++) {
            mask[i] = (tokens[i] == padToken) ? 0L : 1L;
        }
        
        return mask;
    }

    private byte[] loadModelFromAssets() throws IOException {
        InputStream inputStream = context.getAssets().open("model.onnx");
        return inputStream.readAllBytes();
    }

    public void close() {
        try {
            if (session != null) {
                session.close();
            }
            if (env != null) {
                env.close();
            }
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}